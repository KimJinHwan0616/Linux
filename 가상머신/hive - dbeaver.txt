-------------------hive 실행 확인-------------------
su -	# root 계정
bigdata

su hadoop	# hadoop 계정
jps		# 6개 프로세스 확인

## 5개 프로세스 이하
stop-yarn.sh
stop-dfs.sh

start-dfs.sh
start-yarn.sh

jps		# 6개 프로세스 확인
##

su hive		# hive 계정
jps		# RunJar 확인

## RunJar 확인 불가
cd $HIVE_HOME
hive  --service  hiveserver2  &

jps		# RunJar 확인
##

---------------------------------------------------------
dbeaver 실행

# hive 작동과정
hive는 테이블의 정보를 저장하고 관리하기 위해
메타스토어라는 내부 데이터베이스를 사용

즉, 메타스토어에 테이블 스키마를 저장해두고
테이블의 실제 데이터는 hadoop의 HDFS에 저장함

# hive 내부internal테이블 vs 외부external테이블
내부테이블은 테이블과 관련된 데이터를
hive에서 직접 관리함(managed table)

테이블 삭제시 테이블 생성시 만들어진
HDFS 디렉토리까지 함께 제거됨   (중요!!)

반면, 외부테이블은 테이블 정보만 hive에서 관리하고
데이터(HDFS 디렉토리)는 hadoop에서 관리(unmanaged table)

테이블 삭제시 테이블 스키마만 제거되고
데이터(HDFS 디렉토리)는 남아 있음

iris, pima
http://naver.me/FgiP3kF8

-------------------hive 계정 암호 설정-------------------

su -	# root 계정

su hive
passwd   hive	# 비밀번호 : hive
hive
hive

-------------------hive 계정 HDFS 권한 재설정-------------------

su  hive		# hive 계정
hadoop  fs   -chmod   -R   777   .		# /user/hive 이하의 모든 디렉토리에 쓰기 권한 부여

ctrl + d		# root 계정

su hadoop	# hadoop 계정
hadoop  fs   -chmod   -R   777   /tmp	# /tmp 이하의 모든 디렉토리에 쓰기 권한 부여

ctrl + d		# root 계정
su hive

-------------------hive 내부 테이블 실습 (iris.csv)-------------------

1) WinSCP 실행 -> /home/hive ->  iris.csv, indian.csv 복사

2) dbeaver 실행

-- iris 내부 테이블 생성 --
create table iris1 (
    sepal_length  float,
    sepal_width  float,
    petal_length  float,
    petal_width  float,
    spices  string
) row format delimited fields terminated by ','
lines terminated by '\n'
stored as textfile;

-- 헤더 제외 --
alter table iris1
set TBLPROPERTIES('skip.header.line.count' = '1');

-- iris.csv 데이터 가져오기
load data local inpath '/home/hive/iris.csv'
into table iris1;

select * from iris1

3) Putty 실행

HDFS에서 데이터 확인
hadoop   fs   -ls    warehouse/bigdata.db
hadoop   fs   -cat   warehouse/bigdata.db/iris1/iris.csv
hadoop   fs   -cat   warehouse/bigdata.db/iris1/*

DROP table iris1 ;  -- 테이블 삭제 (dbeaver)
hadoop   fs   -ls    warehouse/bigdata.db	# iris1 삭제

-------------------hive 외부 테이블 실습 (hotdogs-winners.csv)-------------------
su  hadoop
hadoop  fs  -ls  .		# sqoop에서 저장한 hotdogs-winners 확인

--  hotdogs 외부 테이블 생성 --
create external table hotdogs (
  year  int, winner string,
  eaten float, country string,
  record int) row format delimited fields terminated by ','
 location '/user/hadoop/hotdog_winners';
